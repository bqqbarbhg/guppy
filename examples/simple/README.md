# Simple

Simple CPU-only example that adds some numbers together.

## Compiling

Compile `main.cpp` as C++14.

```bash
clang++ -std=c++14 -O2 -pthread main.cpp -o example
```

## Walkthrough

### *kernels.h*

Let's start with `kernels.h`, this is the file we use to define compute kernels
that can run multithreaded or on the GPU.

We start off by including `guppy.h` which gives us access to the kernel
programming environment (see [*docs/kernel.md*](docs/kernel.md)).

```cpp
#include "../../guppy.h"
```

Next we define a type for our input numbers. As this is plain C++ it will be
visible to the code in `main.cpp` as well and can be used to pass data from CPU
to GPU.

```cpp
typedef struct {
    int32_t a, b;
} Input;
```

Then we define our kernel `sum_kernel()` with a group size of `(256,1,1)`.

The first argument is of type `gp_global_dim1` which is special: Any value
passed there there will determine the kernel dispatch size (rounded up to the
group size). If you don't have this type of parameter you must determine the
dispatch dimensions manually in `gp::device::dispatch()`.

`gp_buffer()` arguments refer to `gp::buffer` data. On GPU they're just plain
pointers but on CPU they're bounds checked ranges to help you catch bugs.

```cpp
gp_kernel(sum_kernel, 256, 1, 1, gp_args(
    gp_global_dim1 length,
    gp_buffer(const Input) input,
    gp_buffer(int32_t) output))
```

Our kernel implementation needs to use `gp_for_tile()` to execute for each
thread in the thread group. We use the utility `gp_for_tile_in_bounds_1d()`
which is equivalent to checking that the global index is within bounds in the
beginning of the loop.

```cpp
gp_for_tile_in_bounds_1d(length) {
    uint32_t ix = gp_global_index_1d();
    output[ix] = input[ix].a + input[ix].b;
}
```
### *main.cpp*

As Guppy is a single-file library we need to implement it in a single file, we
have only one file so let's just do it here.

```cpp
#define GP_IMPLEMENTATION
#include "../../guppy.h"
```

The kernels are similar, a single file needs to define `GP_KERNEL_IMPL` to
generate the CPU implementations for each kernel.

```cpp
#define GP_KERNEL_IMPL
#include "kernels.h"
```

To do anything with Guppy we need to create `gp::device`, we didn't supply any
`GP_USE_X` defines so only the CPU backend is used.

```cpp
gp::device device = gp::create_device();
```

To send data to the GPU (or CPU in our case..) we have to create a `gp::buffer`,
these can either be initialized with data like `input` or zeroed like `result`
(you can also supply `gp::buffer_desc` for advanced usage, like uninitialized
buffers).

```cpp
gp::buffer<Input> input = device.create_buffer<Input>(N, input_values);
gp::buffer<int32_t> result = device.create_buffer<int32_t>(N);
```

The next line is where the magic happens: This will dispatch the kernel on the
target device. In our case this will spawn threads and execute thread groups
in parallel. `gp::device::dispatch()` takes a `gp::dispatch_desc` argument that
can be retrieved from our kernel function wrappers generated by `gp_kernel()`.

```cpp
device.dispatch(sum_kernel(N, input, result));
```

Finally we must read the data back using `gp::buffer::read()`. This will
automatically block and wait for the preceding dispatches to complete.

```cpp
int32_t result_values[N];
result.read(result_values, 0, N);
```
